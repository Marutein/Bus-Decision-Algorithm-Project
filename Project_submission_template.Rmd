---
title: "6600: Project Submission template"
output:
  html_document: default
  pdf_document: default
---

**Data Sets**

```{r}
project_data <- read.csv("http://data.mishra.us/files/project_data.csv")
head(project_data)
```

```{r}
project_reviews <- read.csv(url("http://data.mishra.us/files/project_reviews.csv"))
head(project_reviews)
```

```{r}
#pre-process the data to prepare for regression
library(tidyverse)

project_data = project_data %>% 
  mutate(
    seen_alone = if_else(seen_alone == 'no',0,1),
    discount = if_else(discount == 'no',0,1),
    job = factor(job),
    education = factor(education)
  )
```


**Regression analysis**

## Question 1

Of the 8 predictors, which predictors have a significant influence on amount spent on concessions? Which predictors are multicollinear? Justify your response with reasons from the analysis.

```{r, warning=FALSE, message=FALSE}
#Linear Regression (Significant Testing)
#note that the lm function takes care of dummytizing
model1<- lm(amount_spent~., data=project_data)
summary(model1)
```

```{r}
#Testing multicollinearity using Variance Inflation Factors (VIF)
library(car)
vif_output <- as.data.frame(car::vif(model1))
vif_output$VIF = vif_output$`GVIF^(1/(2*Df))`**2
vif_output

library("olsrr")
ols_vif_tol(model1)

```

> We find that four vairables have a significant influence on the amount spent on concessions: Age, streaming, days_member, and movies seen. In a linear regression, the coefficients for all of these predictors differ from 0 with greater than 95% confidence, meaning we are quite sure they have a significant influence on the outcome.

> This dataset includes factor variables, which introduces a twist on the standard way we test for multi-collinearity. Since factor variables have more than 1 degree of freedom, we must use the generalized VIF. To then make values for each feature comparable, we must raise this to the 1/2df. The result is analogous to taking the square root of the standard VIF. Here, we square the value to so we can apply our standard rules of thumb. Upon squaring, no values have VIF greater than 5 and we conclude that no features are multicollinear. 

> We also test for multicollinearity using the library olsrr. All tolerance values are above 0.25 and VIF values are below 5, so again we conclude no features are multicollinear.  

Citation:
MsGISRocker (https://stats.stackexchange.com/users/44862/msgisrocker), Which variance inflation factor should I be using: $\text{GVIF}$ or $\text{GVIF}^{1/(2\cdot\text{df})}$?, URL (version: 2020-02-25): https://stats.stackexchange.com/q/96584

## Question 2

Which predictors have a positive influence and which predictors have a negative influence on the amount spent on concessions? Which analysis, regression or penal-ized regression, helped you answer this question? If you ran a neural net model, can it help you find the significant (or not) predictors and their magnitude and direction of influence on the outcome?

```{r}
#we visualize our model output again for clarity
summary(model1)
```

> say which of the 4 are positive/negative
> regular
> ann are black box so no

## Question 3

Given the significant predictors, what strategies can MovieMagic come up with to increase amount spent on concessions?

> answer

**Penalized Regression**

##Question 4 

Which analysis, linear regression or penalized regression, helps you select relevant variables? Which predictor variables would you use in the model? Justify your answer using the analysis. Would a Ridge or a LASSO help in selecting relevant predictors?

```{r, warning=FALSE, message=False}
library(glmnet)
library(caret)

#we start by splitting the data into train and test on a 70-30 split
set.seed(123)
datasplit <- createDataPartition(project_data$amount_spent, p = 0.7, list=FALSE)
trainData <- project_data[datasplit,]
testData  <- project_data[-datasplit,]

#split predictors and outcome
predictors <- trainData[,c(1:8)]
amount_spent <- trainData$amount_spent

#dummytize predictors
predictors <- data.matrix(predictors)

```

```{r}
set.seed(123)

cv.model <- cv.glmnet(x = predictors, 
                         y = amount_spent, 
                         alpha=1,  #use lasso to select
                         family="gaussian", #use guassian for quantitative outcome
                         nfolds=4, 
                         standardize = TRUE, 
                         type.measure = "mse") #use mean squared error since outcome is numeric

plot(cv.model)

best.lambda <- cv.model$lambda.min
best.lambda

 y4<- coef(cv.binomial, s="lambda.min", exact=FALSE)
print(y4)
```



**Predictive model**
The analysis was run by splitting the data........
```{r, warning=FALSE, message=FALSE}


```

**Text Analysis**

**WordCloud**
```{r, warning=FALSE, message=FALSE}
library(corpus)
library(tm)
library(wordcloud)

project_reviews_more_3stars<- subset(project_reviews, star>=3)
project_reviews_less_2stars<- subset(project_reviews, star<=3)

text_more_3stars <-paste(project_reviews_more_3stars$text)
text_more_less_2stars <-paste(project_reviews_less_2stars$text)


#Creating a wordcloud for reviews over 3 stars
corpus <- VCorpus(VectorSource(text_more_3stars))

# we create a function that helps us clean special characters
# like /,@,\\,|,:
ct <- content_transformer(function(x, pattern) gsub(pattern, " ", x))
corpus <- tm_map(corpus, ct, "/|@|\\|:")
# remove white space
corpus<- tm_map(corpus, stripWhitespace)
# the next line of code converts all words to lower case else same
# word as lower and uppercase will be classified as different words
corpus <- tm_map(corpus, content_transformer(tolower))
# remove numbers
corpus <- tm_map(corpus, removeNumbers) 
# remove punctuations
corpus <- tm_map(corpus, removePunctuation) 
# removes common stopwords
corpus <- tm_map(corpus, removeWords, stopwords(kind="en")) 
#term-to-document matrix
dtm <- TermDocumentMatrix(corpus)
# converts document to term matrix object as a matrix
mtrix <- as.matrix(dtm) 
# sorts them in decreasing order
v <- sort(rowSums(mtrix),decreasing=TRUE)
d <- data.frame(word = names(v),freq=v)


wordcloud(words = d$word, freq = d$freq, min.freq = 1,
max.words=100, random.order=FALSE, rot.per=0.35,
colors=brewer.pal(8, "Dark2"), scale=c(3, 0.7))


#Creating a wordcloud for reviews lower than 2 stars

corpus2 <- VCorpus(VectorSource(text_more_less_2stars))
# we create a function that helps us clean special characters
# like /,@,\\,|,:
ct <- content_transformer(function(x, pattern) gsub(pattern, " ", x))
corpus2 <- tm_map(corpus2, ct, "/|@|\\|:")
# remove white space
corpus2<- tm_map(corpus2, stripWhitespace)
# the next line of code converts all words to lower case else same
# word as lower and uppercase will be classified as different words
corpus2 <- tm_map(corpus2, content_transformer(tolower))
# remove numbers
corpus2 <- tm_map(corpus2, removeNumbers) 
# remove punctuations
corpus2 <- tm_map(corpus2, removePunctuation) 
# removes common stopwords
corpus2 <- tm_map(corpus2, removeWords, stopwords(kind="en")) 
#term-to-document matrix
dtm2 <- TermDocumentMatrix(corpus2)
# converts document to term matrix object as a matrix
mtrix2 <- as.matrix(dtm2) 
# sorts them in decreasing order
v <- sort(rowSums(mtrix2),decreasing=TRUE) 
d <- data.frame(word = names(v),freq=v)

#wordcloud settings
wordcloud(words = d$word, freq = d$freq, min.freq = 1,
max.words=100, random.order=FALSE, rot.per=0.35,
colors=brewer.pal(8, "Dark2"), scale=c(3, 0.7))

```

**Topic Modeling**

```{r}
library(topicmodels)
library(tidytext)
library(ggplot2)
library(dplyr)

set.seed(234)

corpus<- project_reviews_less_2stars
corpus <- VCorpus(VectorSource(corpus$text))

data<-
dtm <- DocumentTermMatrix(corpus)


```

