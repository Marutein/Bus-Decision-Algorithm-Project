---
title: "6600: Project Submission template"
output:
  html_document: default
  pdf_document: default
---

**Data Sets**

```{r}
project_data <- read.csv("http://data.mishra.us/files/project_data.csv")
head(project_data)
```

```{r}
project_reviews <- read.csv(url("http://data.mishra.us/files/project_reviews.csv"))
head(project_reviews)
```

```{r}
#pre-process the data to prepare for regression
library(tidyverse)

project_data = project_data %>% 
  mutate(
    seen_alone = if_else(seen_alone == 'no',0,1),
    discount = if_else(discount == 'no',0,1),
    job = factor(job),
    education = factor(education)
  )
```


**Regression analysis**

## Question 1

Of the 8 predictors, which predictors have a significant influence on amount spent on concessions? Which predictors are multicollinear? Justify your response with reasons from the analysis.

```{r, warning=FALSE, message=FALSE}
#Linear Regression (Significant Testing)
#note that the lm function takes care of dummytizing
model1<- lm(amount_spent~., data=project_data)
summary(model1)
```

```{r}
#Testing multicollinearity using Variance Inflation Factors (VIF)
library(car)
vif_output <- as.data.frame(car::vif(model1))
vif_output$VIF = vif_output$`GVIF^(1/(2*Df))`**2
vif_output

library("olsrr")
ols_vif_tol(model1)

```

> We find that four vairables have a significant influence on the amount spent on concessions: Age, streaming, days_member, and movies seen. In a linear regression, the coefficients for all of these predictors differ from 0 with greater than 95% confidence, meaning we are quite sure they have a significant influence on the outcome.

> This dataset includes factor variables, which introduces a twist on the standard way we test for multi-collinearity. Since factor variables have more than 1 degree of freedom, we must use the generalized VIF. To then make values for each feature comparable, we must raise this to the 1/2df. The result is analogous to taking the square root of the standard VIF. Here, we square the value to so we can apply our standard rules of thumb. Upon squaring, no values have VIF greater than 5 and we conclude that no features are multicollinear. 

> We also test for multicollinearity using the library olsrr. All tolerance values are above 0.25 and VIF values are below 5, so again we conclude no features are multicollinear.  

Citation:
MsGISRocker (https://stats.stackexchange.com/users/44862/msgisrocker), Which variance inflation factor should I be using: $\text{GVIF}$ or $\text{GVIF}^{1/(2\cdot\text{df})}$?, URL (version: 2020-02-25): https://stats.stackexchange.com/q/96584

## Question 2

Which predictors have a positive influence and which predictors have a negative influence on the amount spent on concessions? Which analysis, regression or penal-ized regression, helped you answer this question? If you ran a neural net model, can it help you find the significant (or not) predictors and their magnitude and direction of influence on the outcome?

```{r}
#we visualize our model output again for clarity
summary(model1)
```

> say which of the 4 are positive/negative
> regular
> ann are black box so no

## Question 3

Given the significant predictors, what strategies can MovieMagic come up with to increase amount spent on concessions?

> answer

**Penalized Regression**

##Question 4 

Which analysis, linear regression or penalized regression, helps you select relevant variables? Which predictor variables would you use in the model? Justify your answer using the analysis. Would a Ridge or a LASSO help in selecting relevant predictors?

```{r, warning=FALSE, message=False}
library(glmnet)
library(caret)

#we start by splitting the data into train and test on a 70-30 split
set.seed(123)
datasplit <- createDataPartition(project_data$amount_spent, p = 0.7, list=FALSE)
trainData <- project_data[datasplit,]
testData  <- project_data[-datasplit,]

#split predictors and outcome
predictors <- trainData[,c(1:8)]
amount_spent <- trainData$amount_spent

#dummytize predictors
predictors <- data.matrix(predictors)

```

```{r}
set.seed(123)

cv.model <- cv.glmnet(x = predictors, 
                         y = amount_spent, 
                         alpha=1,  #use lasso to select
                         family="gaussian", #use guassian for quantitative outcome
                         nfolds=4, 
                         standardize = TRUE, 
                         type.measure = "mse") #use mean squared error since outcome is numeric

plot(cv.model)

best.lambda <- cv.model$lambda.min
best.lambda

y4<- coef(cv.binomial, s="lambda.min", exact=FALSE)
print(y4)
```



**Predictive model**
The analysis was run by splitting the data........
```{r, warning=FALSE, message=FALSE}


```

**Text Analysis**

**WordCloud**
```{r, warning=FALSE, message=FALSE}
library(corpus)
library(tm)
library(wordcloud)

project_reviews_more_3stars<- subset(project_reviews, star>=3)
project_reviews_less_2stars<- subset(project_reviews, star<=3)

text_more_3stars <-paste(project_reviews_more_3stars$text)
text_more_less_2stars <-paste(project_reviews_less_2stars$text)


#Creating a wordcloud for reviews over 3 stars
corpus <- VCorpus(VectorSource(text_more_3stars))

# we create a function that helps us clean special characters
# like /,@,\\,|,:
ct <- content_transformer(function(x, pattern) gsub(pattern, " ", x))
corpus <- tm_map(corpus, ct, "/|@|\\|:")
# remove white space
corpus<- tm_map(corpus, stripWhitespace)
# the next line of code converts all words to lower case else same
# word as lower and uppercase will be classified as different words
corpus <- tm_map(corpus, content_transformer(tolower))
# remove numbers
corpus <- tm_map(corpus, removeNumbers) 
# remove punctuations
corpus <- tm_map(corpus, removePunctuation) 
# removes common stopwords
corpus <- tm_map(corpus, removeWords, stopwords(kind="en")) 
#term-to-document matrix
dtm <- TermDocumentMatrix(corpus)
# converts document to term matrix object as a matrix
mtrix <- as.matrix(dtm) 
# sorts them in decreasing order
v <- sort(rowSums(mtrix),decreasing=TRUE)
d <- data.frame(word = names(v),freq=v)


wordcloud(words = d$word, freq = d$freq, min.freq = 1,
max.words=100, random.order=FALSE, rot.per=0.35,
colors=brewer.pal(8, "Dark2"), scale=c(3, 0.7))


#Creating a wordcloud for reviews lower than 2 stars

corpus2 <- VCorpus(VectorSource(text_more_less_2stars))
# we create a function that helps us clean special characters
# like /,@,\\,|,:
ct <- content_transformer(function(x, pattern) gsub(pattern, " ", x))
corpus2 <- tm_map(corpus2, ct, "/|@|\\|:")
# remove white space
corpus2<- tm_map(corpus2, stripWhitespace)
# the next line of code converts all words to lower case else same
# word as lower and uppercase will be classified as different words
corpus2 <- tm_map(corpus2, content_transformer(tolower))
# remove numbers
corpus2 <- tm_map(corpus2, removeNumbers) 
# remove punctuations
corpus2 <- tm_map(corpus2, removePunctuation) 
# removes common stopwords
corpus2 <- tm_map(corpus2, removeWords, stopwords(kind="en")) 
#term-to-document matrix
dtm2 <- TermDocumentMatrix(corpus2)
# converts document to term matrix object as a matrix
mtrix2 <- as.matrix(dtm2) 
# sorts them in decreasing order
v <- sort(rowSums(mtrix2),decreasing=TRUE) 
d <- data.frame(word = names(v),freq=v)

#wordcloud settings
wordcloud(words = d$word, freq = d$freq, min.freq = 1,
max.words=100, random.order=FALSE, rot.per=0.35,
colors=brewer.pal(8, "Dark2"), scale=c(3, 0.7))

```
## Question 6 
Knowing the prominent words in each of the wordclouds, what strategies can be developed in messaging customers?
> We want to create an outstanding experience for our customers; if food and movies are intrinsically related, we must ensure that they are in an enjoyable environment where they can enjoy the film. We also need to provide great food.
Based on the wordcloud, we can infer that the establishment is already a good place for the consumer so we will focus on the relationship between food, service, and experience.
1) Discounts on tickets:
If we grant discounts on tickets, we will increase consumer attendance, allowing us to focus our market on food consumption.
2) Orders by apps:
Order food, snacks, and drinks through mobile applications to save waiting times in lines.
3) Special purchases:
Request drinks and snacks during the movie to be brought to the client's seat. (Extra charge)
4) Food Lockers:
The implementation of food lockers could allow users to buy before and during the movie and pick it up directly without having to wait. (No extra charge)
5) Combo deals:
If the customer purchases a combo, they can get a free refill of any items that make up it.
6) Full-service meal:
Expand conventional options to offer complete meals to customers.
7) Reward consumption:
When reaching a specific expense in the consumption of food, snacks, and drinks, we can grant an improvement in the seats that the clients will use on the next visit; another option will be to give them a free food pass on their next visit.

Would the strategies differ?
> Analyzing both word clouds, we can identify that food and movies are entirely related. The strategy won't differ.


**Topic Modeling**

```{r}
library(topicmodels)
library(tidytext)
library(ggplot2)
library(dplyr)

dtm3 <- DocumentTermMatrix(corpus2)

rowTotals <- apply(dtm3 , 1, sum)

set.seed(234)

lda <- LDA(dtm3, k = 3, method = "Gibbs", control = NULL)
topics <- tidy(lda, matrix = "beta")


top_terms <- topics %>%
group_by(topic) %>%
top_n(10, beta) %>% # top_n picks 10 topics.
ungroup() %>%
arrange(topic, -beta)

top_terms %>%
mutate(term = reorder(term, beta)) %>%
ggplot(aes(term, beta, fill = factor(topic))) +
geom_col(show.legend = FALSE) +
facet_wrap(~ topic, scales = "free") +
coord_flip()
```

##Question 7
Which term is the most relevant in each of the three topics and how would it inform your business strategy?
> ANSWER

What strategies would you suggest are possible for MovieMagic if it wants to increase concession sales.
> ANSWER

Would you recommend promotions or advertising or loyalty program; justify your choice of business strategy?
> ANSWER
