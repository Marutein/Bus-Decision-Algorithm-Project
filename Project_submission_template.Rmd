---
title: "6600: Project Submission template"
output:
  html_document: default
  pdf_document: default
---

**Data Sets**

```{r}
project_data <- read.csv("http://data.mishra.us/files/project_data.csv")
head(project_data)
```

```{r}
project_reviews <- read.csv(url("http://data.mishra.us/files/project_reviews.csv"))
head(project_reviews)
```

```{r}
#pre-process the data to prepare for regression
library(tidyverse)

project_data = project_data %>% 
  mutate(
    seen_alone = if_else(seen_alone == 'no',0,1),
    discount = if_else(discount == 'no',0,1),
    job = factor(job),
    education = factor(education)
  )
```


**Regression analysis**

## Question 1

Of the 8 predictors, which predictors have a significant influence on amount spent on concessions? Which predictors are multicollinear? Justify your response with reasons from the analysis.

```{r, warning=FALSE, message=FALSE}
#Linear Regression (Significant Testing)
#note that the lm function takes care of dummytizing
model1<- lm(amount_spent~., data=project_data)
summary(model1)
```

```{r}
#Testing multicollinearity using Variance Inflation Factors (VIF)
library(car)
vif_output <- as.data.frame(car::vif(model1))
vif_output$VIF = vif_output$`GVIF^(1/(2*Df))`**2
vif_output

library("olsrr")
ols_vif_tol(model1)

```

> We find that four vairables have a significant influence on the amount spent on concessions: Age, streaming, days_member, and movies seen. In a linear regression, the coefficients for all of these predictors differ from 0 with greater than 95% confidence, meaning we are quite sure they have a significant influence on the outcome.

> This dataset includes factor variables, which introduces a twist on the standard way we test for multi-collinearity. Since factor variables have more than 1 degree of freedom, we must use the generalized VIF. To then make values for each feature comparable, we must raise this to the 1/2df. The result is analogous to taking the square root of the standard VIF. Here, we square the value to so we can apply our standard rules of thumb. Upon squaring, no values have VIF greater than 5 and we conclude that no features are multicollinear. 

> We also test for multicollinearity using the library olsrr. All tolerance values are above 0.25 and VIF values are below 5, so again we conclude no features are multicollinear.  

Citation:
MsGISRocker (https://stats.stackexchange.com/users/44862/msgisrocker), Which variance inflation factor should I be using: $\text{GVIF}$ or $\text{GVIF}^{1/(2\cdot\text{df})}$?, URL (version: 2020-02-25): https://stats.stackexchange.com/q/96584

## Question 2

Which predictors have a positive influence and which predictors have a negative influence on the amount spent on concessions? Which analysis, regression or penal-ized regression, helped you answer this question? If you ran a neural net model, can it help you find the significant (or not) predictors and their magnitude and direction of influence on the outcome?

```{r}
#we visualize our model output again for clarity
summary(model1)
```

> say which of the 4 are positive/negative
> regular
> ann are black box so no

## Question 3

Given the significant predictors, what strategies can MovieMagic come up with to increase amount spent on concessions?

> answer

**Penalized Regression**

##Question 4 

Which analysis, linear regression or penalized regression, helps you select relevant variables? Which predictor variables would you use in the model? Justify your answer using the analysis. Would a Ridge or a LASSO help in selecting relevant predictors?

```{r}
# we first need to dummytize our factor variables since penalized regression requires numeric inputs
library(fastDummies)
project_data_dummy <- dummy_cols(project_data, 
                                 select_columns = c('job','education'), #dummytize our factor variables
                                 remove_selected_columns = TRUE) #remove the original columns

project_data_dummy <- subset(project_data_dummy, 
                             select = -c(job_unknown,education_unknown) #remove one dummy to avoid multicollinearity. Remove unknown as so it's the reference column 
                             )
```


```{r, warning=FALSE, message=False}
library(glmnet)
library(caret)

#we start by splitting the data into train and test on a 70-30 split
set.seed(123)
datasplit <- createDataPartition(project_data_dummy$amount_spent, p = 0.7, list=FALSE)
trainData <- project_data_dummy[datasplit,]
testData  <- project_data_dummy[-datasplit,]

#split predictors and outcome
predictors <- subset(trainData,
                     select = -c(amount_spent))
amount_spent <- trainData$amount_spent

#convert to matrix for glmnet function
predictors <- data.matrix(predictors)

```

```{r}
#running the model

#set seed for cross validation
set.seed(123)

cv.model <- cv.glmnet(x = predictors, 
                         y = amount_spent, 
                         alpha=1,  #use lasso to select
                         family="gaussian", #use guassian for quantitative outcome
                         nfolds=4, #4 fold validation
                         standardize = TRUE, #standardize inputs
                         type.measure = "mse") #use mean squared error since outcome is numeric

#see plot for optimal lambda
plot(cv.model)
#output the best value for lambda
best.lambda <- cv.model$lambda.min
best.lambda
```

```{r}
#See which coefficients are shrunk to 0 using optimal lambda
coefficients <- coef(cv.model, s="lambda.min", exact=FALSE)
print(coefficients)
```

> Linear regression does not have a shrinkage parameter and thus does not redue the effect of variables. Penalized regression, however, does shrink the impact of variables and helps us select relevant preditors variables. Particularly, LASSO regression can reduce the impact of variables all the way to 0. Those variables reduced to 0 can be eliminated from the analysis. 

> In the code above, we first create dummy variables for factor variables sincee penalized regression requires numeric inputs. Next, we split our dummytized data into a test and train set. Next, we run a penalized regression model. We choose a LASSO model so it will reduce coefficients to 0. We also select a Gaussian model since we have a numeric output and chose to use Mean Squared Error (MSE) as our measure for model accuracy. We also implement cross-validation to select the optimal shrinkage parameter (lambda) that minimizes our MSE. Finally, use that optimal lambda value to run our best model. 

> In the best model, several variables are reduced to 0 importance. Thus, in our final model we would use only: age, streaming, discount, days_member, movies_seen, job_entrepreneur, job_hospitality, job_retired, job_self-employed, and education_secondary.

**Predictive model**

##Question 5

If you split the data 70-30 versus 80-20, how does it influence i) relevant variables selected, ii) RMSE and R-squared values of the linear regression?

```{r}
#We will evaluate our models on RMSE and R^2

#define rmse
rmse <- function(actual, predicted) sqrt(mean((actual - predicted)^2))

#define r^2
r2 <- function(actual, predicted){
  TSS <- sum((actual - mean(actual))^2)
  RSS <- sum((actual - predicted)^2)
  1 - RSS/TSS
}

```

```{r}
#results of 70-30

#prepare test data 
test_predictors <- subset(testData,
                     select = -c(amount_spent))
test_actual <- testData$amount_spent
test_predictors <- data.matrix(test_predictors)

# using the penalized regression model for predicion on the test data
test_predictions = predict(cv.model, 
                           newx = test_predictors, 
                           type = "response", #predictions on scale of resonse variable
                           s ="lambda.min")

#calculate rmse
rmse <- rmse(test_actual, test_predictions)

#calculate r^2
r2 <- r2(test_actual, test_predictions)

print('For the 70-30 split, our model has the following performance:')
print(paste('RMSE: ', rmse))
print(paste('R-Squared ',r2))

```

```{r}
#whole analysis for 80-20 split

#split 80-20
set.seed(123)
datasplit <- createDataPartition(project_data_dummy$amount_spent, p = 0.8, list=FALSE)
trainData <- project_data_dummy[datasplit,]
testData  <- project_data_dummy[-datasplit,]
#split predictors and outcome
predictors <- subset(trainData,
                     select = -c(amount_spent))
amount_spent <- trainData$amount_spent
#convert to matrix for glmnet function
predictors <- data.matrix(predictors)
#set seed for cross validation
set.seed(123)
cv.model <- cv.glmnet(x = predictors, 
                         y = amount_spent, 
                         alpha=1,  #use lasso to select
                         family="gaussian", #use guassian for quantitative outcome
                         nfolds=4, #4 fold validation
                         standardize = TRUE, #standardize inputs
                         type.measure = "mse") #use mean squared error since outcome is numeric
#See which coefficients are shrunk to 0 using optimal lambda
coefficients <- coef(cv.model, s="lambda.min", exact=FALSE)
print(coefficients)

```

```{r}
#results of 80-20
#prepare test data 
test_predictors <- subset(testData,
                     select = -c(amount_spent))
test_actual <- testData$amount_spent
test_predictors <- data.matrix(test_predictors)
# using the penalized regression model for predicion on the test data
test_predictions = predict(cv.model, 
                           newx = test_predictors, 
                           type = "response", #predictions on scale of resonse variable
                           s ="lambda.min")
#calculate rmse
rmse <- rmse(test_actual, test_predictions)
#calculate r^2
r2 <- r2(test_actual, test_predictions)

print('For the 80-20 split, our model has the following performance:')
print(paste('RMSE: ', rmse))
print(paste('R-Squared ',r2))
```

If you split the data 70-30 versus 80-20, how does it influence i) relevant variables selected, ii) RMSE and R-squared values of the linear regression?

> When we split the data 80-20, we drop two more variables (job_entrepreneur and education_secondary) since their effect is now insignificant. the other variables we included in the 70-30 split are also included in the 80-20. No variables are included in the 80-20 that were not incuded in the 70-30.

> When we split the data 70-30, our rmse is 9.666 and our model explains 65.8% of the variance in amount_spent in the test set. During the 80-20 split our model improves. Our rmse drops to 9.631 and our R-squared improves to 66.9%. Our model improves in both out-of-sample statistics. This means, we'd choose the use the 80-20 model for future prediction. Using more train data means our model has more bias (ability to pick up idiosyncrecies in the train data) but also less variance (it's less likely to perform well on new data). However, since our out-of-sample statistics improved, we can be confident the 80-20 model will do well with new data.
**Text Analysis**

**WordCloud**
```{r, warning=FALSE, message=FALSE}
library(corpus)
library(tm)
library(wordcloud)

project_reviews_more_3stars<- subset(project_reviews, star>=3)
project_reviews_less_2stars<- subset(project_reviews, star<=3)

text_more_3stars <-paste(project_reviews_more_3stars$text)
text_more_less_2stars <-paste(project_reviews_less_2stars$text)


#Creating a wordcloud for reviews over 3 stars
corpus <- VCorpus(VectorSource(text_more_3stars))

# we create a function that helps us clean special characters
# like /,@,\\,|,:
ct <- content_transformer(function(x, pattern) gsub(pattern, " ", x))
corpus <- tm_map(corpus, ct, "/|@|\\|:")
# remove white space
corpus<- tm_map(corpus, stripWhitespace)
# the next line of code converts all words to lower case else same
# word as lower and uppercase will be classified as different words
corpus <- tm_map(corpus, content_transformer(tolower))
# remove numbers
corpus <- tm_map(corpus, removeNumbers) 
# remove punctuations
corpus <- tm_map(corpus, removePunctuation) 
# removes common stopwords
corpus <- tm_map(corpus, removeWords, stopwords(kind="en")) 
#term-to-document matrix
dtm <- TermDocumentMatrix(corpus)
# converts document to term matrix object as a matrix
mtrix <- as.matrix(dtm) 
# sorts them in decreasing order
v <- sort(rowSums(mtrix),decreasing=TRUE)
d <- data.frame(word = names(v),freq=v)


wordcloud(words = d$word, freq = d$freq, min.freq = 1,
max.words=100, random.order=FALSE, rot.per=0.35,
colors=brewer.pal(8, "Dark2"), scale=c(3, 0.7))


#Creating a wordcloud for reviews lower than 2 stars

corpus2 <- VCorpus(VectorSource(text_more_less_2stars))
# we create a function that helps us clean special characters
# like /,@,\\,|,:
ct <- content_transformer(function(x, pattern) gsub(pattern, " ", x))
corpus2 <- tm_map(corpus2, ct, "/|@|\\|:")
# remove white space
corpus2<- tm_map(corpus2, stripWhitespace)
# the next line of code converts all words to lower case else same
# word as lower and uppercase will be classified as different words
corpus2 <- tm_map(corpus2, content_transformer(tolower))
# remove numbers
corpus2 <- tm_map(corpus2, removeNumbers) 
# remove punctuations
corpus2 <- tm_map(corpus2, removePunctuation) 
# removes common stopwords
corpus2 <- tm_map(corpus2, removeWords, stopwords(kind="en")) 
#term-to-document matrix
dtm2 <- TermDocumentMatrix(corpus2)
# converts document to term matrix object as a matrix
mtrix2 <- as.matrix(dtm2) 
# sorts them in decreasing order
v <- sort(rowSums(mtrix2),decreasing=TRUE) 
d <- data.frame(word = names(v),freq=v)

#wordcloud settings
wordcloud(words = d$word, freq = d$freq, min.freq = 1,
max.words=100, random.order=FALSE, rot.per=0.35,
colors=brewer.pal(8, "Dark2"), scale=c(3, 0.7))

```
## Question 6 
Knowing the prominent words in each of the wordclouds, what strategies can be developed in messaging customers?
> We want to create an outstanding experience for our customers; if food and movies are intrinsically related, we must ensure that they are in an enjoyable environment where they can enjoy the film. We also need to provide great food.
Based on the wordcloud, we can infer that the establishment is already a good place for the consumer so we will focus on the relationship between food, service, and experience.
1) Discounts on tickets:
If we grant discounts on tickets, we will increase consumer attendance, allowing us to focus our market on food consumption.
2) Orders by apps:
Order food, snacks, and drinks through mobile applications to save waiting times in lines.
3) Special purchases:
Request drinks and snacks during the movie to be brought to the client's seat. (Extra charge)
4) Food Lockers:
The implementation of food lockers could allow users to buy before and during the movie and pick it up directly without having to wait. (No extra charge)
5) Combo deals:
If the customer purchases a combo, they can get a free refill of any items that make up it.
6) Full-service meal:
Expand conventional options to offer complete meals to customers.
7) Reward consumption:
When reaching a specific expense in the consumption of food, snacks, and drinks, we can grant an improvement in the seats that the clients will use on the next visit; another option will be to give them a free food pass on their next visit.

Would the strategies differ?
> Analyzing both word clouds, we can identify that food and movies are entirely related. The strategy won't differ.


**Topic Modeling**

```{r}
library(topicmodels)
library(tidytext)
library(ggplot2)
library(dplyr)

dtm3 <- DocumentTermMatrix(corpus2)

rowTotals <- apply(dtm3 , 1, sum)

set.seed(234)

lda <- LDA(dtm3, k = 3, method = "Gibbs", control = NULL)
topics <- tidy(lda, matrix = "beta")


top_terms <- topics %>%
group_by(topic) %>%
top_n(10, beta) %>% # top_n picks 10 topics.
ungroup() %>%
arrange(topic, -beta)

top_terms %>%
mutate(term = reorder(term, beta)) %>%
ggplot(aes(term, beta, fill = factor(topic))) +
geom_col(show.legend = FALSE) +
facet_wrap(~ topic, scales = "free") +
coord_flip()
```

##Question 7
Which term is the most relevant in each of the three topics and how would it inform your business strategy?
>TOPIC 1: This topic focuses on how good food is in the cinema.
TOPIC 2: It talks about how enjoyable it is to watch a movie in the cinema.
TOPIC 3: It is about the service quality that the cinema offer.

What strategies would you suggest are possible for MovieMagic if it wants to increase concession sales.
> ANSWER

Would you recommend promotions or advertising or loyalty program; justify your choice of business strategy?
> ANSWER
